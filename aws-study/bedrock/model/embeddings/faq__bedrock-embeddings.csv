Question,Answer,Tag
"What are embeddings in AWS Bedrock?","Embeddings are numerical vector representations of data (text, images, documents) that capture semantic meaning in a high-dimensional space. Similar concepts are placed closer together, enabling semantic search, RAG, classification, and clustering.","Embeddings,General"
"Which embedding models are available in Bedrock?","Bedrock offers: Cohere Embed v4 (multimodal, 128K context), Cohere Embed 3 English/Multilingual (1024 dims), Amazon Titan Text Embeddings V2 (256-1024 dims), Titan Multimodal Embeddings G1, Amazon Nova Multimodal Embeddings, and TwelveLabs Marengo Embed 3.0/2.7 for video.","Embeddings,Models"
"Does the embedding model matter when choosing a foundation model?","Yes, but they operate independently. The embedding model converts data to vectors for search, while the FM generates responses. You can mix providers (e.g., Cohere embeddings + Claude FM). Use the same embedding model for indexing AND querying.","Embeddings,RAG"
"Can I use different embedding models with different foundation models?","Yes. Embedding models and foundation models are independent. You can use Cohere embeddings with Claude, Titan embeddings with Nova, etc. The key is consistency: use the same embedding model for indexing and querying.","Embeddings,RAG"
"What is the difference between Cohere Embed v4 and v3?","Cohere Embed v4 supports 128K context (vs 512 tokens in v3), configurable dimensions (256-1536), interleaved text+image inputs, and better document understanding (tables, charts, handwriting). V3 is simpler for basic text/image embedding.","Embeddings,Cohere"
"When should I use Amazon Titan vs Cohere embeddings?","Use Titan V2 for: widest regional availability (20+ regions), cost efficiency, text-only use cases. Use Cohere v4 for: complex documents with tables/charts, multimodal content, industry-specific (finance/healthcare) documents.","Embeddings,Model Selection"
"What are binary embeddings and why use them?","Binary embeddings represent vectors as 1-bit values instead of 32-bit floats, reducing storage by 32x. Trade-off: ~5-10% lower retrieval accuracy. Use for high-volume, cost-sensitive apps with reranking. Not for precision-critical use cases.","Embeddings,Binary"
"Why are binary embeddings not used by default?","Binary embeddings lose semantic precision: 4.3 billion possible values per dimension reduced to 2 (0/1). This causes ~7% recall drop, lost nuance between similar concepts, and worse ranking quality. Float is default for maximum accuracy.","Embeddings,Binary"
"What dimensions should I use for embeddings?","256 dims = lowest cost, good for high-volume. 512 = balanced. 1024 = standard RAG. 1536 = maximum accuracy. Higher dimensions = better quality but more storage/compute cost. Start with 1024 and optimize based on benchmarks.","Embeddings,Dimensions"
"How do I migrate between embedding models?","You must re-embed your entire corpus. Vectors from different models are not compatibleâ€”they exist in different vector spaces. Plan for re-indexing time and cost when switching models.","Embeddings,Migration"
"Which embedding models support Knowledge Bases?","All Bedrock embedding models support Knowledge Bases: Cohere Embed v4, Cohere Embed 3, Titan Text Embeddings V2, and Titan Multimodal Embeddings. Choose based on data type and regional requirements.","Embeddings,Knowledge Bases"
"What is the difference between search_document and search_query input types?","For Cohere models: use search_document when indexing your corpus for retrieval, use search_query when encoding user queries. This adds special tokens to optimize for each use case. Mixing them causes suboptimal retrieval.","Embeddings,Cohere"
"How is embedding pricing calculated in Bedrock?","Embeddings are charged per input token processed. There is no output token cost. Titan V2: $0.00002/1K tokens. Cohere Embed 3: $0.0001/1K tokens. Multimodal pricing includes per-image charges.","Embeddings,Pricing"
"What vector databases work with Bedrock Knowledge Bases?","Bedrock Knowledge Bases supports: Amazon OpenSearch Serverless, Amazon Aurora PostgreSQL (pgvector), Pinecone, Redis Enterprise Cloud, and MongoDB Atlas.","Embeddings,Knowledge Bases"
"Can I use embeddings for image search?","Yes. Use Cohere Embed v4 (best for documents with images), Cohere Embed 3 with multimodal support, Titan Multimodal Embeddings G1, or Amazon Nova Multimodal Embeddings for text+image semantic search.","Embeddings,Multimodal"
"What is the two-stage retrieval pattern for embeddings?","Use binary embeddings for fast initial retrieval (Top 100), then rerank with float embeddings or Cohere Rerank for precision (Top 10), then pass to LLM. This balances cost and quality.","Embeddings,Architecture"
"What is the maximum context length for embedding models?","Cohere Embed v4: 128K tokens. Cohere Embed v3: 512 tokens. Titan Text Embeddings V2: 8K tokens. For RAG, smaller chunks (512-2048 tokens) often improve retrieval regardless of context limit.","Embeddings,Context"
"Which regions support Cohere Embed v4?","Single-region: us-east-1, eu-west-1, ap-northeast-1. Cross-region inference available in 24+ regions. Titan V2 has widest coverage with 20+ regions including GovCloud.","Embeddings,Regional"
"What is TwelveLabs Marengo and when should I use it?","TwelveLabs Marengo Embed is a video-native embedding model for video search, classification, and content analysis. Use Marengo 3.0 for up to 4 hours/6GB video with 36 languages. Use for video-first applications, sports analysis, media libraries.","Embeddings,TwelveLabs,Video"
"How does video embedding differ from text/image embedding?","Video embedding uses async inference (not synchronous like text/images) due to processing time. TwelveLabs Marengo generates multiple specialized vectors per video capturing visual, audio, and temporal dimensions. Charged per second of video.","Embeddings,Video"
"Can I search videos using text queries?","Yes. TwelveLabs Marengo supports cross-modal semantic search: embed videos with Marengo, then query using natural language text or images. The model maps both to the same vector space for similarity matching.","Embeddings,TwelveLabs"
