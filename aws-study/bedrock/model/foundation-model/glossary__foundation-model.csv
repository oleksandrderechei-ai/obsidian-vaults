Term,Summary,Description
"Foundation Model (FM)","Large pre-trained neural network","A large neural network trained on massive, generalized datasets capable of performing a wide variety of tasks out of the box. Used as a starting point for building AI applications quickly and cost-effectively."
"Large Language Model (LLM)","Text-specialized foundation model","A type of foundation model specifically trained on text data to understand and generate human language. Examples: Claude, GPT-4, Llama, Nova."
"Transformer","Neural network architecture","The dominant architecture for modern foundation models. Uses attention mechanisms to process sequences and understand relationships between tokens. Powers most LLMs and vision models."
"Token","Basic unit of text processing","A chunk of text (typically 3-4 characters or a word) that models process. Pricing is based on input/output tokens. Context windows are measured in tokens."
"Context Window","Maximum input length","The maximum number of tokens a model can process in a single request. Larger windows allow processing longer documents. E.g., Claude: 200K, Nova Pro: 300K tokens."
"Inference","Running a trained model","The process of using a trained model to generate predictions or outputs from new inputs. This is what happens when you send a prompt to Bedrock."
"On-Demand Inference","Pay-per-use model access","Serverless access where you pay only for tokens processed. AWS maintains a shared pool of models. Analogy: Uber - you use it, pay, and leave."
"Provisioned Throughput","Reserved model capacity","Dedicated compute capacity reserved for your use. Pay hourly rate 24/7 regardless of usage. Analogy: Private driver - you pay even when idle."
"Model Unit","Unit of provisioned capacity","A measure of reserved compute capacity for Provisioned Throughput. More units = higher throughput for your dedicated workload."
"Prompt","Input to a foundation model","The text, image, or other input you send to a model to generate a response. Quality of prompts significantly affects output quality."
"Prompt Engineering","Crafting effective prompts","The practice of designing prompts to guide model behavior and get better outputs. Often more effective than fine-tuning for many use cases."
"Few-Shot Learning","Learning from examples in prompt","Providing examples in your prompt to help the model understand the desired output format or task. No retraining required."
"In-Context Learning","Learning during inference","The ability of foundation models to adapt to new tasks based on examples provided in the prompt, without updating model weights."
"Fine-Tuning","Customizing a pre-trained model","Training a foundation model on your specific dataset to improve performance for your use case. Creates a custom model version."
"RAG","Retrieval Augmented Generation","A technique that retrieves relevant external knowledge and provides it to the model to improve accuracy and reduce hallucinations."
"Hallucination","False but plausible output","When a model generates information that sounds correct but is factually wrong. Mitigated with RAG, guardrails, and careful validation."
"Guardrails","Safety and policy controls","Filters and controls that block harmful content, enforce policies, and ensure model outputs meet safety requirements. Bedrock provides built-in guardrails."
"Self-Supervised Learning","Learning without labels","Training method where models learn patterns from raw data without manual labeling. Models predict missing parts of data (next token, masked words)."
"Multimodal","Multiple input/output types","Models that can process and/or generate multiple types of content: text, images, video, audio. E.g., Nova Pro accepts text/image/video input."
"Text-to-Image","Generate images from text","Capability to create images based on text descriptions. Models: Stable Diffusion, Nova Canvas, DALL-E."
"Bedrock Runtime","AWS model hosting layer","The AWS-managed infrastructure that hosts foundation models. Completely separate from your AWS account. Handles model deployment and scaling."
"Bedrock Gateway","API routing layer","The regional endpoint (e.g., bedrock-runtime.us-east-1.amazonaws.com) that receives your requests and routes them to model deployments."
"VPC Endpoint","Private AWS connectivity","AWS PrivateLink connection that allows Bedrock access without traversing the public internet. Enhances security and data isolation."
"Model Invocation Logging","Optional prompt/response logging","Bedrock feature to log prompts and responses for auditing. Disabled by default. Logs go to your S3 bucket or CloudWatch when enabled."
"Escrow Account","Third-party model hosting","AWS-managed accounts where third-party provider software (like Anthropic, Cohere) runs. Providers cannot access your logs or data."
"TLS","Transport Layer Security","Encryption protocol (TLS 1.2+) that secures data in transit between your application and Bedrock. All Bedrock traffic is TLS-encrypted."
"Customer Managed Keys (CMK)","Your own encryption keys","AWS KMS keys you control for encrypting data at rest in Knowledge Bases or Agents. Provides additional security control."
"Knowledge Cutoff","Training data end date","The date up to which a model's training data extends. Affects knowledge of recent events. Check model docs for specific dates."
"Bias","Inherited prejudices in models","Systematic errors or prejudices in model outputs inherited from training data. Requires testing, monitoring, and guardrails to mitigate."
"Chaining","Sequential model calls","Combining multiple model calls in sequence to accomplish complex workflows. Output of one call becomes input to the next."
"Streaming","Real-time token delivery","Receiving model output tokens as they're generated rather than waiting for complete response. Improves perceived latency for users."
"Service Tiers","Bedrock pricing/performance levels","Different levels of service (Priority/Standard/Flex) with different latency, throughput, and cost characteristics. Match tier to workload needs."
